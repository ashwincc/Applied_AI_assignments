{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"jKMeoHjWN-3k"},"source":["# <b>Assignment : DT</b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"w0vpJMx2pHYX"},"source":["<font color='red'><b> Please check below video before attempting this assignment</b></font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IrJVk4Chpzjp"},"outputs":[],"source":["from IPython.display import YouTubeVideo\n","YouTubeVideo('ZhLXULFjIjQ', width=\"1000\",height=\"500\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CjA-ZU-TqVK1"},"source":["<font color='red'><b> TF-IDFW2V</b></font>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uvBr2z6iqW9V"},"source":["<b>Tfidf w2v (w1,w2..) = (tfidf(w1) * w2v(w1) + tfidf(w2) * w2v(w2) + …)  /    (tfidf(w1) + tfidf(w2) + …)</b>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"zRAy5UzOvi_a"},"source":["<b>(Optional) Please check course video on [AVgw2V and TF-IDFW2V ](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2916/avg-word2vec-tf-idf-weighted-word2vec/3/module-3-foundations-of-natural-language-processing-and-machine-learning)for more details."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"IB2uk5LwtBlO"},"source":["<font color='blue'><b>Glove vectors </b></font>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"j697XLZGtCnz"},"source":["<b>In this assignment you will be working with glove vectors , please check  [this](https://en.wikipedia.org/wiki/GloVe_(machine_learning)) and [this](https://en.wikipedia.org/wiki/GloVe_(machine_learning)) for more details.</b><br>\n","\n","Download glove vectors from this [link ](https://drive.google.com/file/d/1lDca_ge-GYO0iQ6_XDLWePQFMdAA2b8f/view?usp=sharing)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"_ufHLoACuoHw"},"outputs":[],"source":["import pickle\n","#please use below code to load glove vectors \n","with open('glove_vectors', 'rb') as f:\n","    model = pickle.load(f)\n","    glove_words =  set(model.keys())"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"YMurhntCAooj"},"source":["or else , you can use below code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8-KyNeiQAtLG"},"outputs":[],"source":["'''\n","# Reading glove vectors in python: https://stackoverflow.com/a/38230349/4084039\n","def loadGloveModel(gloveFile):\n","    print (\"Loading Glove Model\")\n","    f = open(gloveFile,'r', encoding=\"utf8\")\n","    model = {}\n","    for line in tqdm(f):\n","        splitLine = line.split()\n","        word = splitLine[0]\n","        embedding = np.array([float(val) for val in splitLine[1:]])\n","        model[word] = embedding\n","    print (\"Done.\",len(model),\" words loaded!\")\n","    return model\n","model = loadGloveModel('glove.42B.300d.txt')\n","\n","# ============================\n","Output:\n","    \n","Loading Glove Model\n","1917495it [06:32, 4879.69it/s]\n","Done. 1917495  words loaded!\n","\n","# ============================\n","\n","words = []\n","for i in preproced_texts:\n","    words.extend(i.split(' '))\n","\n","for i in preproced_titles:\n","    words.extend(i.split(' '))\n","print(\"all the words in the coupus\", len(words))\n","words = set(words)\n","print(\"the unique words in the coupus\", len(words))\n","\n","inter_words = set(model.keys()).intersection(words)\n","print(\"The number of words that are present in both glove vectors and our coupus\", \\\n","      len(inter_words),\"(\",np.round(len(inter_words)/len(words)*100,3),\"%)\")\n","\n","words_courpus = {}\n","words_glove = set(model.keys())\n","for i in words:\n","    if i in words_glove:\n","        words_courpus[i] = model[i]\n","print(\"word 2 vec length\", len(words_courpus))\n","\n","\n","# stronging variables into pickle files python: http://www.jessicayung.com/how-to-use-pickle-to-save-and-load-variables-in-python/\n","\n","import pickle\n","with open('glove_vectors', 'wb') as f:\n","    pickle.dump(words_courpus, f)\n","\n","\n","'''"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"OTJ7Et5hxpZS"},"source":["# <font color='red'> <b>Task - 1</b></font>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ACUkHex3N-3m"},"source":["<ol>\n","    <li><strong>Apply Decision Tree Classifier(DecisionTreeClassifier) on these feature sets</strong>\n","        <ul>\n","            <li><font color='red'>Set 1</font>: categorical, numerical features +  preprocessed_essay (TFIDF) + Sentiment scores(preprocessed_essay)</li>\n","            <li><font color='red'>Set 2</font>: categorical, numerical features +  preprocessed_essay (TFIDF W2V) + Sentiment scores(preprocessed_essay)</li>        </ul>\n","    </li>\n","    <li><strong>The hyper paramter tuning (best `depth` in range [1, 3, 10, 30], and the best `min_samples_split` in range [5, 10, 100, 500])</strong>\n","        <ul>\n","    <li>Find the best hyper parameter which will give the maximum <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/receiver-operating-characteristic-curve-roc-curve-and-auc-1/'>AUC</a> value</li>\n","    <li>find the best hyper paramter using k-fold cross validation(use gridsearch cv or randomsearch cv)/simple cross validation data(you can write your own for loops refer sample solution)</li>\n","        </ul>\n","    </li>\n","    <li>\n","    <strong>Representation of results</strong>\n","        <ul>\n","    <li>You need to plot the performance of model both on train data and cross validation data for each hyper parameter, like shown in the figure\n","    <img src='https://i.imgur.com/Gp2DQmh.jpg' width=500px> with X-axis as <strong>min_sample_split</strong>, Y-axis as <strong>max_depth</strong>, and Z-axis as <strong>AUC Score</strong> , we have given the notebook which explains how to plot this 3d plot, you can find it in the same drive <i>3d_scatter_plot.ipynb</i></li>\n","            <p style=\"text-align:center;font-size:30px;color:red;\"><strong>or</strong></p> <br>\n","    <li>You need to plot the performance of model both on train data and cross validation data for each hyper parameter, like shown in the figure\n","    <img src='https://i.imgur.com/fgN9aUP.jpg' width=300px> <a href='https://seaborn.pydata.org/generated/seaborn.heatmap.html'>seaborn heat maps</a> with rows as <strong>min_sample_split</strong>, columns as <strong>max_depth</strong>, and values inside the cell representing <strong>AUC Score</strong> </li>\n","    <li>You choose either of the plotting techniques out of 3d plot or heat map</li>\n","    <li>Once after you found the best hyper parameter, you need to train your model with it, and find the AUC on test data and plot the ROC curve on both train and test.\n","        Make sure that you are using predict_proba method to calculate AUC curves, because AUC is calcualted on class probabilities and not on class labels.\n","    <img src='https://i.imgur.com/wMQDTFe.jpg' width=300px></li>\n","    <li>Along with plotting ROC curve, you need to print the <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/confusion-matrix-tpr-fpr-fnr-tnr-1/'>confusion matrix</a> with predicted and original labels of test data points\n","    <img src='https://i.imgur.com/IdN5Ctv.png' width=300px></li>\n","    <li>Once after you plot the confusion matrix with the test data, get all the `false positive data points`\n","        <ul>\n","            <li> Plot the WordCloud(https://www.geeksforgeeks.org/generating-word-cloud-python/) with the words of essay text of these `false positive data points`</li>\n","            <li> Plot the box plot with the `price` of these `false positive data points`</li>\n","            <li> Plot the pdf with the `teacher_number_of_previously_posted_projects` of these `false positive data points`</li>\n","        </ul>\n","        </ul>\n","    </li>\n","   "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MmNrc-8piqK-"},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"aqWyfo1Sx8ua"},"source":["# <font color='red'><b> Task - 2 </b></font>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Xddr3kChx-ew"},"source":["For this task consider **set-1** features.\n","\n","*  Select all the features which are having non-zero feature importance.You can get the feature importance using  'feature_importances_` \n","   (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), discard the all other remaining features and then apply any of the model of you choice i.e. (Dession tree, Logistic Regression, Linear SVM).\n","*  You need to do hyperparameter tuning corresponding to the model you selected and procedure in step 2 and step 3<br>\n","  **Note**: when you want to find the feature importance make sure you don't use max_depth parameter keep it None.\n","  </li>\n","    <br>\n","You need to summarize the results at the end of the notebook, summarize it in the table format\n","        <img src='http://i.imgur.com/YVpIGGE.jpg' width=400px>\n","    </li>\n","</ol>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"oZ-qDp6KxNj0"},"source":["<font color='blue'><b>Hint for calculating Sentiment scores</b></font>"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"2IHTExN4xd5p"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package vader_lexicon to\n","[nltk_data]     /home/jordan/nltk_data...\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","nltk.download('vader_lexicon')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"YKZIvFYBxaaD","outputId":"d9dcffbf-971d-4220-c03e-bf01cd81bdd4"},"outputs":[{"name":"stdout","output_type":"stream","text":["sentiment score for sentence 1 {'neg': 0.0, 'neu': 0.213, 'pos': 0.787, 'compound': 0.5719}\n","sentiment score for sentence 2 {'neg': 0.756, 'neu': 0.244, 'pos': 0.0, 'compound': -0.4767}\n","sentiment score for sentence 3 {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"]}],"source":["import nltk\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","# import nltk\n","# nltk.download('vader_lexicon')\n","\n","sid = SentimentIntensityAnalyzer()\n","\n","sample_sentence_1='I am happy.'\n","ss_1 = sid.polarity_scores(sample_sentence_1)\n","print('sentiment score for sentence 1',ss_1)\n","\n","sample_sentence_2='I am sad.'\n","ss_2 = sid.polarity_scores(sample_sentence_2)\n","print('sentiment score for sentence 2',ss_2)\n","\n","sample_sentence_3='I am going to New Delhi tommorow.'\n","ss_3 = sid.polarity_scores(sample_sentence_3)\n","print('sentiment score for sentence 3',ss_3)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["0.787"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["ss_1['pos']"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"luEzcFrGiqLa"},"source":["# <font color='red'> <b>Task - 1</b></font>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"QQmid3VAN-31"},"source":["## 1.1 Loading Data"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from scipy.sparse import hstack\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import roc_auc_score,roc_curve,auc,confusion_matrix\n","from sklearn.model_selection import train_test_split\n","import seaborn as sn \n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","# from wordcloud import WordCloud, STOPWORDS\n","# from tabulate import tabulate"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"NqY4ES_3N-33"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>school_state</th>\n","      <th>teacher_prefix</th>\n","      <th>project_grade_category</th>\n","      <th>teacher_number_of_previously_posted_projects</th>\n","      <th>project_is_approved</th>\n","      <th>clean_categories</th>\n","      <th>clean_subcategories</th>\n","      <th>essay</th>\n","      <th>price</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ca</td>\n","      <td>mrs</td>\n","      <td>grades_prek_2</td>\n","      <td>53</td>\n","      <td>1</td>\n","      <td>math_science</td>\n","      <td>appliedsciences health_lifescience</td>\n","      <td>i fortunate enough use fairy tale stem kits cl...</td>\n","      <td>725.05</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ut</td>\n","      <td>ms</td>\n","      <td>grades_3_5</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>specialneeds</td>\n","      <td>specialneeds</td>\n","      <td>imagine 8 9 years old you third grade classroo...</td>\n","      <td>213.03</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  school_state teacher_prefix project_grade_category   \n","0           ca            mrs          grades_prek_2  \\\n","1           ut             ms             grades_3_5   \n","\n","   teacher_number_of_previously_posted_projects  project_is_approved   \n","0                                            53                    1  \\\n","1                                             4                    1   \n","\n","  clean_categories                 clean_subcategories   \n","0     math_science  appliedsciences health_lifescience  \\\n","1     specialneeds                        specialneeds   \n","\n","                                               essay   price  \n","0  i fortunate enough use fairy tale stem kits cl...  725.05  \n","1  imagine 8 9 years old you third grade classroo...  213.03  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["#make sure you are loading atleast 50k datapoints\n","#you can work with features of preprocessed_data.csv for the assignment.\n","import pandas\n","data = pandas.read_csv('preprocessed_data.csv')\n","data.head(2)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>teacher_number_of_previously_posted_projects</th>\n","      <th>project_is_approved</th>\n","      <th>price</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>109248.000000</td>\n","      <td>109248.000000</td>\n","      <td>109248.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>11.153165</td>\n","      <td>0.848583</td>\n","      <td>298.119343</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>27.777154</td>\n","      <td>0.358456</td>\n","      <td>367.498030</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.660000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>104.310000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>2.000000</td>\n","      <td>1.000000</td>\n","      <td>206.220000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>9.000000</td>\n","      <td>1.000000</td>\n","      <td>379.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>451.000000</td>\n","      <td>1.000000</td>\n","      <td>9999.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       teacher_number_of_previously_posted_projects  project_is_approved   \n","count                                 109248.000000        109248.000000  \\\n","mean                                      11.153165             0.848583   \n","std                                       27.777154             0.358456   \n","min                                        0.000000             0.000000   \n","25%                                        0.000000             1.000000   \n","50%                                        2.000000             1.000000   \n","75%                                        9.000000             1.000000   \n","max                                      451.000000             1.000000   \n","\n","               price  \n","count  109248.000000  \n","mean      298.119343  \n","std       367.498030  \n","min         0.660000  \n","25%       104.310000  \n","50%       206.220000  \n","75%       379.000000  \n","max      9999.000000  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["data.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cT7V9hz7iqLm"},"outputs":[],"source":["# write your code in following steps for task 1\n","# 1. calculate sentiment scores for the essay feature \n","# 2. Split your data.\n","# 3. perform tfidf vectorization of text data.\n","# 4. perform tfidf w2v vectorization of text data.\n","# 5. perform encoding of categorical features.\n","# 6. perform encoding of numerical features\n","# 7. For task 1 set 1 stack up all the features\n","# 8. For task 1 set 2 stack up all the features (for stacking dense features you can use np.stack)\n","# 9. Perform hyperparameter tuning and plot either heatmap or 3d plot.\n","# 10. Find the best parameters and fit the model. Plot ROC-AUC curve(using predict proba method)\n","# 11. Plot confusion matrix based on best threshold value\n","# 12. Find all the false positive data points and plot wordcloud of essay text and pdf of teacher_number_of_previously_posted_projects.\n","# 13. Write your observations about the wordcloud and pdf."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["1.1 Calculate sentiment scores"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["data['pos_sentiment'] = 0\n","data['neg_sentiment'] = 0\n","data['neu_sentiment'] = 0\n","data['compound_sentiment'] = 0"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["for idx, essay in enumerate(data['essay']):\n","    sid_score = sid.polarity_scores(essay)\n","    data.at[idx, 'pos_sentiment'] = sid_score['pos']\n","    data.at[idx, 'neg_sentiment'] = sid_score['neg']\n","    data.at[idx, 'neu_sentiment'] = sid_score['neu']\n","    data.at[idx, 'compound_sentiment'] = sid_score['compound']\n","    # break"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["X = data.drop(columns='project_is_approved', axis=1)\n","y = data['project_is_approved']"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["1.2 Split your data"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train shape: (76473, 12); Test Shape: (32775, 12)\n"]}],"source":["x_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=42)\n","print(f'Train shape: {x_train.shape}; Test Shape: {x_test.shape}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["1.3 TFIDF vectorizer"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["tfidf = TfidfVectorizer()\n","tfidf_train = tfidf.fit_transform(x_train['essay'])a"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"data":{"text/plain":["<87398x51643 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 9442301 stored elements in Compressed Sparse Row format>"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["tfidf_train"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["1.4 TFIDF w2v"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["import pickle\n","with open ('glove_vectors', \"rb\") as f:\n","    model = pickle.load(f)\n","    glove_words = set(model.keys())"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[{"data":{"text/plain":["49058"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["tfidf_model = TfidfVectorizer()\n","tfidf_model.fit(x_train['essay'])\n","# we are converting a dictionary with word as a key, and the idf as a value\n","dictionary = dict(zip(tfidf_model.get_feature_names_out(), list(tfidf_model.idf_)))\n","tfidf_words = set(tfidf_model.get_feature_names_out())\n","len(tfidf_words)"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 76473/76473 [00:54<00:00, 1400.62it/s]"]},{"name":"stdout","output_type":"stream","text":["76473\n","300\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# average Word2Vec\n","# compute average word2vec for each review.\n","tfidf_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n","for sentence in tqdm(x_train['essay']): # for each review/sentence\n","    vector = np.zeros(300) # as word vectors are of zero length\n","    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n","    for word in sentence.split(): # for each word in a review/sentence\n","        if (word in glove_words) and (word in tfidf_words):\n","            vec = model[word] # getting the vector for each word\n","            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n","            tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n","            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n","            tf_idf_weight += tf_idf\n","    if tf_idf_weight != 0:\n","        vector /= tf_idf_weight\n","    tfidf_w2v_vectors.append(vector)\n","\n","print(len(tfidf_w2v_vectors))\n","print(len(tfidf_w2v_vectors[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mHvBlI_9N-4X"},"outputs":[],"source":["# please write all the code with proper documentation, and proper titles for each subsection\n","# go through documentations and blogs before you start coding\n","# first figure out what to do, and then think about how to do.\n","# reading and understanding error messages will be very much helpfull in debugging your code\n","# when you plot any graph make sure you use \n","    # a. Title, that describes your plot, this will be very helpful to the reader\n","    # b. Legends if needed\n","    # c. X-axis label\n","    # d. Y-axis label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qCT9iOq7iqLu"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MeSKca6viqLw"},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"XmHih9-YiqLx"},"source":["# <font color='red'> <b>Task - 2</b></font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zk2-az7QiqLz"},"outputs":[],"source":["# 1. write your code in following steps for task 2\n","# 2. select all non zero features\n","# 3. Update your dataset i.e. X_train,X_test and X_cv so that it contains all rows and only non zero features\n","# 4. perform hyperparameter tuning and plot either heatmap or 3d plot.\n","# 5. Fit the best model. Plot ROC AUC curve and confusion matrix similar to model 1.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N44iBzrmiqL1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WRDi4wd8iqL2"},"outputs":[],"source":["# Tabulate your results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fSVKa3RviqL3"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Assignment_DT_Instructions.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
